{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhqLDfze0HzdOxKXRFBzd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/canonical_llm_impl/blob/main/LLM_from_scratch_chap_02_Process_Text_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial for \"LLM from Scratch\" Chapter 02\n",
        "\n",
        "https://drive.google.com/drive/u/1/folders/1a9jbhCJr_dddOT-m-4G9MgBTpOdaCs7Q"
      ],
      "metadata": {
        "id": "Pe_YqB_lP8cP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oC2sPpujL9Vc",
        "outputId": "a1332690-53cd-4419-f598-623e3b06a395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uv\n",
            "  Downloading uv-0.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.7.0\n",
            "/bin/bash: line 1: !uv: command not found\n"
          ]
        }
      ],
      "source": [
        "# @title Install Dependencies\n",
        "!pip install uv && !uv pip install --system -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download training data text"
      ],
      "metadata": {
        "id": "iccQYws2nDcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "\"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UhCzVUyQI-4",
        "outputId": "27397eaa-ef51-45b8-de80-f26c0e6cd54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the-verdict.txt', <http.client.HTTPMessage at 0x7e4089b8e650>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def get_tokens(text: str):\n",
        "  preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "  preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "  return preprocessed\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "tokens = get_tokens(raw_text)\n",
        "print(f\"# of words: {len(tokens)}\")\n",
        "print(f\"{tokens[:100]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5tu0mPsR8Fl",
        "outputId": "5bb8b261-fd0a-4fe8-e712-356c8f92216e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of words: 4690\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Vocab"
      ],
      "metadata": {
        "id": "3XbzAtbhmOsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Simple Vocab V1\n",
        "\n",
        "all_words = sorted(set(tokens))\n",
        "vocab_size = len(all_words)\n",
        "print(f\"{vocab_size=}\")\n",
        "\n",
        "vocab: dict[str, int] = {w:index for index, w in enumerate(all_words)}\n",
        "for i in range(11):\n",
        "  print(f\"{all_words[i]} -> {vocab[all_words[i]]}\")\n",
        "\n",
        "class SimpleTokenzierV1:\n",
        "  \"\"\"Do not support tokens not seen in vocab (OOD).\"\"\"\n",
        "\n",
        "  def __init__(self, vocab: dict[str, int]):\n",
        "    \"\"\"Ctor.\n",
        "\n",
        "    Args:\n",
        "        vocab: String (token) to int mapping.\n",
        "    \"\"\"\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {v:k for k, v in vocab.items()}\n",
        "\n",
        "  def encode(self, text: str) -> list[int]:\n",
        "    tokens = get_tokens(text)\n",
        "    return [self.str_to_int[token] for token in tokens]\n",
        "\n",
        "  def decode(self, tokens: list[int]) -> str:\n",
        "    text = \" \".join([self.int_to_str[it] for it in tokens])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "# Test\n",
        "simple_tokenizer_v1 = SimpleTokenzierV1(vocab)\n",
        "text = \"\"\"It's the last he painted, you know, Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "encoded = simple_tokenizer_v1.encode(text)\n",
        "print(f\"{encoded=}\")\n",
        "decoded = simple_tokenizer_v1.decode(encoded)\n",
        "print(f\"{decoded=}\")\n",
        "print(f\"{text=}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "yMpKVgM9nvcE",
        "outputId": "b2e6c75f-2280-4751-b322-b2bebbcd1ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size=1130\n",
            "! -> 0\n",
            "\" -> 1\n",
            "' -> 2\n",
            "( -> 3\n",
            ") -> 4\n",
            ", -> 5\n",
            "-- -> 6\n",
            ". -> 7\n",
            ": -> 8\n",
            "; -> 9\n",
            "? -> 10\n",
            "encoded=[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
            "decoded=\"It' s the last he painted, you know, Mrs. Gisburn said with pardonable pride.\"\n",
            "text=\"It's the last he painted, you know, Mrs. Gisburn said with pardonable pride.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Simple Vocab V2\n",
        "\n",
        "all_words = sorted(set(tokens))\n",
        "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab_size = len(all_words)\n",
        "print(f\"{vocab_size=}\")\n",
        "\n",
        "vocab: dict[str, int] = {w:index for index, w in enumerate(all_words)}\n",
        "for i in range(len(all_words) - 10, len(all_words)):\n",
        "  print(f\"{all_words[i]} -> {vocab[all_words[i]]}\")\n",
        "\n",
        "class SimpleTokenzierV2:\n",
        "  \"\"\"Compared to V1, this supports extra things:\n",
        "  1) OOD. <|unk|>\n",
        "  2) EOS (end of sequence). <|endoftext|>\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab: dict[str, int]):\n",
        "    \"\"\"Ctor.\n",
        "\n",
        "    Args:\n",
        "        vocab: String (token) to int mapping.\n",
        "    \"\"\"\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {v:k for k, v in vocab.items()}\n",
        "\n",
        "  def encode(self, text: str) -> list[int]:\n",
        "    tokens = get_tokens(text)\n",
        "    tokens = [t if t in self.str_to_int else \"<|unk|>\" for t in tokens]\n",
        "    return [self.str_to_int[token] for token in tokens]\n",
        "\n",
        "  def decode(self, tokens: list[int]) -> str:\n",
        "    text = \" \".join([self.int_to_str[it] for it in tokens])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "# Test\n",
        "simple_tokenizer_v2 = SimpleTokenzierV2(vocab)\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)\n",
        "encoded = simple_tokenizer_v2.encode(text)\n",
        "print(f\"{encoded=}\")\n",
        "decoded = simple_tokenizer_v2.decode(encoded)\n",
        "print(f\"{decoded=}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Mlks0VmEw4xW",
        "outputId": "03b43ffd-1971-4b4a-c8aa-1519b73e49de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size=1132\n",
            "year -> 1122\n",
            "years -> 1123\n",
            "yellow -> 1124\n",
            "yet -> 1125\n",
            "you -> 1126\n",
            "younger -> 1127\n",
            "your -> 1128\n",
            "yourself -> 1129\n",
            "<|endoftext|> -> 1130\n",
            "<|unk|> -> 1131\n",
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
            "encoded=[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
            "decoded='<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BPE Vocab (Byte pair encoding)\n",
        "\n",
        "Used by GPT2/3. E.g. \"tiktoken\"\n",
        "\n",
        "BPE builds its vocabulary by iteratively merging frequent characters into sub-\n",
        "words and frequent subwords into words."
      ],
      "metadata": {
        "id": "A-2zam-m4kxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install tiktoken\n",
        "\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(f\"{version('tiktoken')=}\")\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(f\"{tokenizer.name=}\")\n",
        "\n",
        "text = (\n",
        "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "\"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(f\"{integers=}\")\n",
        "\n",
        "decoded = tokenizer.decode(integers)\n",
        "print(f\"{decoded=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xIuTZ0Q4gJH",
        "outputId": "a0165842-c5ce-43d1-b6df-d22c999b82a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m7 packages\u001b[0m \u001b[2min 366ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 132ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
            "version('tiktoken')='0.9.0'\n",
            "tokenizer.name='gpt2'\n",
            "integers=[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
            "decoded='Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Training dataset and dataloader\n",
        "\n",
        "- Use BPE tokenizer."
      ],
      "metadata": {
        "id": "nhHoogIs-l-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(f\"{len(encoded)=}\")\n",
        "\n",
        "enc_sample = enc_text[50:]\n",
        "\n",
        "context_size = 4\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: \\t{y}\\n\")\n",
        "\n",
        "for i in range(context_size):\n",
        "  context = x[:i+1]\n",
        "  target = y[i]\n",
        "  # print(f\"{context} ---------> {target}\")\n",
        "  print(f\"{tokenizer.decode(context)} ---------> {tokenizer.decode([target])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjGbegiTDfz6",
        "outputId": "fa70f858-2eff-4e1c-8c0c-dc3394c27b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(encoded)=16\n",
            "x: [290, 4920, 2241, 287]\n",
            "y: \t[4920, 2241, 287, 257]\n",
            "\n",
            " and --------->  established\n",
            " and established --------->  himself\n",
            " and established himself --------->  in\n",
            " and established himself in --------->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Use Pytorch Dataset and DataLoader\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      self.input_ids.append(torch.tensor(token_ids[i:i+max_length]))\n",
        "      # TODO(lizhi): does the rhs idx exceed the limit?\n",
        "      self.target_ids.append(torch.tensor(token_ids[i+1:i+max_length+1]))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_ids[index], self.target_ids[index]\n",
        "\n",
        "# Test\n",
        "dataset = GPTDatasetV1(raw_text, tokenizer, 8, 1)\n",
        "print(f\"{len(dataset)=}\")\n",
        "context, target = dataset[2]\n",
        "context = tokenizer.decode(context.numpy())\n",
        "target = tokenizer.decode(target.numpy())\n",
        "print(f\"{context=}  {target=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGNsHEXGKmt4",
        "outputId": "92b0769a-79e6-41d0-ef40-90d7505298ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(dataset)=5137\n",
            "context='AD always thought Jack Gisburn rather'  target=' always thought Jack Gisburn rather a'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "  return dataloader\n",
        "\n",
        "# Test\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=max_length, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(f\"{first_batch=}\\n\")\n",
        "second_batch = next(data_iter)\n",
        "print(f\"\\n{second_batch=}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdgX1v64765V",
        "outputId": "9953a05f-0a05-4ecf-f35d-1d80b252ffac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first_batch=[tensor([[  40,  367, 2885, 1464],\n",
            "        [ 367, 2885, 1464, 1807]]), tensor([[ 367, 2885, 1464, 1807],\n",
            "        [2885, 1464, 1807, 3619]])]\n",
            "\n",
            "\n",
            "second_batch=[tensor([[2885, 1464, 1807, 3619],\n",
            "        [1464, 1807, 3619,  402]]), tensor([[1464, 1807, 3619,  402],\n",
            "        [1807, 3619,  402,  271]])]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Embedding"
      ],
      "metadata": {
        "id": "H8K38z4o2G_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123) # reproducibility\n",
        "vocab_size = tokenizer.n_vocab\n",
        "print(f\"{vocab_size=}\")\n",
        "\n",
        "EMBEDDING_DIM = 256\n",
        "\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "print(f\"{embedding_layer.weight.shape=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao1uM65n2LBc",
        "outputId": "75b9e51b-1b81-47ef-94f7-57b85955da27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size=50257\n",
            "embedding_layer.weight.shape=torch.Size([50257, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer(first_batch[0]).shape # [B, S, H_DIM]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhRHPUJq20fB",
        "outputId": "24b9ac93-5b15-4fe7-f35a-d31f8efff901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(first_batch[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uvs5ag7314M",
        "outputId": "0ed9ec0f-50f4-407b-be9f-f69c1f7d80a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0640,  0.3317,  0.1070,  ...,  0.5349, -0.8024, -2.3238],\n",
            "         [-0.3525,  0.3509,  0.9873,  ..., -1.8466, -1.7034,  0.3223],\n",
            "         [ 1.0017,  0.9299, -1.2633,  ..., -1.2256,  1.1179,  0.1343],\n",
            "         [ 0.7996,  2.2837, -0.6525,  ..., -1.1217,  0.4706,  0.1531]],\n",
            "\n",
            "        [[-0.3525,  0.3509,  0.9873,  ..., -1.8466, -1.7034,  0.3223],\n",
            "         [ 1.0017,  0.9299, -1.2633,  ..., -1.2256,  1.1179,  0.1343],\n",
            "         [ 0.7996,  2.2837, -0.6525,  ..., -1.1217,  0.4706,  0.1531],\n",
            "         [-0.1082, -1.2723, -1.2217,  ..., -0.9199,  2.0073, -1.4138]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding\n",
        "\n",
        "- OpenAI’s GPT models use **absolute** positional embeddings that are **optimized during the training** rather than being fixed or predefined like the positional\n",
        "encodings in the original transformer model"
      ],
      "metadata": {
        "id": "YkNzGWJICRow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, EMBEDDING_DIM)\n",
        "positions = torch.arange(context_length)\n",
        "print(f\"{positions=}\")\n",
        "print(f\"{pos_embedding_layer(positions).shape=}\")\n",
        "\n",
        "pos_embeddings = pos_embedding_layer(positions)\n",
        "print(f\"{pos_embeddings.shape=}\")\n",
        "\n",
        "assert pos_embeddings.shape == (context_length, EMBEDDING_DIM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLRJe1aSCX99",
        "outputId": "b41b1a7d-6450-4104-f19a-ff6a7469e966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positions=tensor([0, 1, 2, 3])\n",
            "pos_embedding_layer(positions).shape=torch.Size([4, 256])\n",
            "pos_embeddings.shape=torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert embedding_layer(first_batch[0]).shape[1:] == pos_embeddings.shape\n",
        "assert (embedding_layer(first_batch[0]) + pos_embeddings).shape == embedding_layer(first_batch[0]).shape"
      ],
      "metadata": {
        "id": "tfAmSOcPEiCi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}