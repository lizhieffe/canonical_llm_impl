{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOb2O4k8I9pla4byeIyGQi/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/canonical_llm_impl/blob/main/LLM_from_scratch_chap_03_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial for \"LLM from Scratch\" Chapter 03\n",
        "\n",
        "https://drive.google.com/drive/u/1/folders/1a9jbhCJr_dddOT-m-4G9MgBTpOdaCs7Q"
      ],
      "metadata": {
        "id": "Pe_YqB_lP8cP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oC2sPpujL9Vc",
        "outputId": "f52c9e57-b213-4f82-e396-6e68056e5e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uv\n",
            "  Downloading uv-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.7.2\n",
            "/bin/bash: line 1: !uv: command not found\n"
          ]
        }
      ],
      "source": [
        "# @title Install Dependencies\n",
        "!pip install uv && !uv pip install --system -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "knx4K6u2El2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Attention Calculation\n",
        "\n",
        "In this section, we calculate a simple version of ATTN.\n",
        "- The Q, K, V are the same vector.\n",
        "\n",
        "- Use **softmax** as normalization:\n",
        "  1. It handles extreme values well\n",
        "  2. The output is positive and can be directly used as probability\n"
      ],
      "metadata": {
        "id": "FCtkGKqaEfbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Impl with iteration\n",
        "\n",
        "# Calculate the attention from inputs[1] to other tokens. (Q * K)\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your\n",
        "     [0.55, 0.87, 0.66], # journey (x^2)\n",
        "     [0.57, 0.85, 0.64], # starts\n",
        "     [0.22, 0.58, 0.33], # with\n",
        "     [0.77, 0.25, 0.10], # one\n",
        "     [0.05, 0.80, 0.55]] # step\n",
        ")\n",
        "print(f\"inputs: {inputs.shape}\")\n",
        "\n",
        "query = inputs[1]\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "print(f\"{attn_scores_2.shape=}\")\n",
        "print()\n",
        "\n",
        "for i, input in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(query, input)\n",
        "\n",
        "# Normalization Option 1: Naive\n",
        "# This doesn't handle extreme value well\n",
        "attn_scores_2_naive_normalized = attn_scores_2 / attn_scores_2.sum()\n",
        "print(f\"{attn_scores_2_naive_normalized=}\")\n",
        "assert attn_scores_2_naive_normalized.sum().numpy() - 1 < 1e-4\n",
        "print()\n",
        "\n",
        "# Normalization Option 2: Softmax\n",
        "# This handles extreme value well\n",
        "attn_scores_2_softmax_normalized = torch.nn.functional.softmax(attn_scores_2, dim=-1)\n",
        "print(f\"{attn_scores_2_softmax_normalized=}\")\n",
        "assert attn_scores_2_softmax_normalized.sum().numpy() - 1 < 1e-4\n",
        "\n",
        "# Calculate the context vector ((Q * K) * V)\n",
        "query = inputs[1]\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i in range(len(inputs)):\n",
        "  context_vec_2 += attn_scores_2_softmax_normalized[i] * inputs[i]\n",
        "print(f\"{context_vec_2=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSw7rqPJEhRw",
        "outputId": "66b404d6-5472-4304-d95b-923e0af6d1ae",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: torch.Size([6, 3])\n",
            "attn_scores_2.shape=torch.Size([6])\n",
            "\n",
            "attn_scores_2_naive_normalized=tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "\n",
            "attn_scores_2_softmax_normalized=tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "context_vec_2=tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Impl with MatMul\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your\n",
        "     [0.55, 0.87, 0.66], # journey (x^2)\n",
        "     [0.57, 0.85, 0.64], # starts\n",
        "     [0.22, 0.58, 0.33], # with\n",
        "     [0.77, 0.25, 0.10], # one\n",
        "     [0.05, 0.80, 0.55]] # step\n",
        ")\n",
        "Q = K = V = inputs\n",
        "print(f\"{Q.shape=}\")\n",
        "\n",
        "attn = Q @ K.transpose(-1, -2) # [N, N]\n",
        "attn = torch.nn.functional.softmax(attn, dim=-1)\n",
        "for i in range(len(attn)):\n",
        "  assert torch.isclose(attn[i].sum(), torch.tensor(1.0))\n",
        "assert attn.shape == (6, 6)\n",
        "\n",
        "y = attn @ V # [N, D]\n",
        "assert y.shape == (6, 3)\n",
        "print(f\"{y[1]=}\")\n",
        "assert torch.allclose(y[1], context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmtS2Q0pg5Rj",
        "outputId": "c90af7f4-6475-48ad-edba-50fd463fa32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q.shape=torch.Size([6, 3])\n",
            "y[1]=tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainable ATTN\n",
        "\n",
        "- Normalize by HIDDEN_DIM ** 0.5 is to improve the **training performance**. When the HIDDEN_DIM increases, the softmax values are more likely to be very small and cause vanishing gradient."
      ],
      "metadata": {
        "id": "cq-bml99kYVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM = 3\n",
        "HIDDEN_DIM = 256\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW-8Xyiokgzl",
        "outputId": "4b641b46-5a90-48ad-c4a5-df85c86106a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c38bc6efa90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Impl version 1\n",
        "\n",
        "wk = torch.nn.Parameter(torch.randn(EMB_DIM, HIDDEN_DIM), requires_grad=False)  # [E, H]\n",
        "wq = torch.nn.Parameter(torch.randn(EMB_DIM, HIDDEN_DIM), requires_grad=False)\n",
        "wv = torch.nn.Parameter(torch.randn(EMB_DIM, HIDDEN_DIM), requires_grad=False)\n",
        "\n",
        "keys = inputs @ wk    # [N, H]\n",
        "queries = inputs @ wq # [N, H]\n",
        "values = inputs @ wv  # [N, H]\n",
        "assert keys.shape == (inputs.shape[0], HIDDEN_DIM)\n",
        "\n",
        "attn = keys @ queries.transpose(-1, -2) # [N, N]\n",
        "attn /= HIDDEN_DIM ** 0.5\n",
        "attn = torch.nn.functional.softmax(attn, dim=-1)\n",
        "assert attn.shape == (inputs.shape[0], inputs.shape[0])\n",
        "for i in range(len(attn)):\n",
        "  assert torch.isclose(attn[i].sum(), torch.tensor(1.0))\n",
        "\n",
        "y = attn @ values # [N, H]\n",
        "assert y.shape == (inputs.shape[0], HIDDEN_DIM)"
      ],
      "metadata": {
        "id": "DsjFefdi8Mev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Impl version 2\n",
        "\n",
        "wk = torch.nn.Linear(EMB_DIM, HIDDEN_DIM)\n",
        "wq = torch.nn.Linear(EMB_DIM, HIDDEN_DIM)\n",
        "wv = torch.nn.Linear(EMB_DIM, HIDDEN_DIM)\n",
        "\n",
        "K = wk(inputs) # [N, H]\n",
        "Q = wq(inputs) # [N, H]\n",
        "V = wv(inputs) # [N, H]\n",
        "assert K.shape == (inputs.shape[0], HIDDEN_DIM)\n",
        "\n",
        "attn = K @ Q.transpose(-1, -2) # [N, N]\n",
        "attn /= HIDDEN_DIM ** 0.5\n",
        "attn = torch.nn.functional.softmax(attn, dim=-1)\n",
        "assert attn.shape == (inputs.shape[0], inputs.shape[0])\n",
        "for i in range(len(attn)):\n",
        "  assert torch.isclose(attn[i].sum(), torch.tensor(1.0))\n",
        "\n",
        "y = attn @ V # [N, H]\n",
        "assert y.shape == (inputs.shape[0], HIDDEN_DIM)"
      ],
      "metadata": {
        "id": "ZZ0lrI1e7DFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title impl as python class - V1\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    \"\"\"Ctor.\n",
        "\n",
        "    Args:\n",
        "      d_in: Input (embedding) dimension.\n",
        "      d_out: Output (hidden) dimension.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.wk = torch.nn.Parameter(torch.randn(d_in, d_out))  # [E, H]\n",
        "    self.wq = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
        "    self.wv = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
        "\n",
        "  def forward(self, x):\n",
        "    assert x.shape[-1] == self.wk.shape[0]\n",
        "\n",
        "    keys = x @ self.wk   # [N, H]\n",
        "    queries = x @ self.wq # [N, H]\n",
        "    values = x @ self.wv  # [N, H]\n",
        "\n",
        "    attn = keys @ queries.transpose(-1, -2) # [N, N]\n",
        "    attn /= HIDDEN_DIM ** 0.5\n",
        "    attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "    res = attn @ values # [N, H]\n",
        "    return res\n",
        "\n",
        "# Test\n",
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(EMB_DIM, 2)\n",
        "y = sa_v1(inputs)\n",
        "assert y.shape == (inputs.shape[0], 2)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8pp4foy9orl",
        "outputId": "99c1741f-820a-48d1-bb3d-f2462589e17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2867, 0.3880],\n",
              "        [0.2868, 0.3881],\n",
              "        [0.2868, 0.3881],\n",
              "        [0.2868, 0.3874],\n",
              "        [0.2868, 0.3869],\n",
              "        [0.2868, 0.3878]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## impl as python class - V2\n",
        "\n",
        "Compared to V1, the V2 uses nn.Linear() instead of raw nn.Parameter(). The benefits are:\n",
        "1. Linear() has higher computation efficiency when there is no bias.\n",
        "2. Linear() has more optimized parameter init scheme, leading to more stable and effective training."
      ],
      "metadata": {
        "id": "pEnyI8zwB0YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    \"\"\"Ctor.\n",
        "\n",
        "    Args:\n",
        "      d_in: Input (embedding) dimension.\n",
        "      d_out: Output (hidden) dimension.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.wk = torch.nn.Linear(d_in, d_out, bias=qkv_bias)  # [E, H]\n",
        "    self.wq = torch.nn.Linear(d_in, d_out, bias=qkv_bias)  # [E, H]\n",
        "    self.wv = torch.nn.Linear(d_in, d_out, bias=qkv_bias)  # [E, H]\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = self.wk(x)   # [N, H]\n",
        "    queries = self.wq(x) # [N, H]\n",
        "    values = self.wv(x)  # [N, H]\n",
        "\n",
        "    attn = keys @ queries.transpose(-1, -2) # [N, N]\n",
        "    attn /= HIDDEN_DIM ** 0.5\n",
        "    attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "    res = attn @ values # [N, H]\n",
        "    return res\n",
        "\n",
        "# Test\n",
        "torch.manual_seed(123)\n",
        "sa_v2 = SelfAttention_v2(EMB_DIM, 2)\n",
        "y = sa_v2(inputs)\n",
        "assert y.shape == (inputs.shape[0], 2)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYdpyeb-_u5n",
        "outputId": "c2c6beaf-fd78-47c0-ca42-eb369a2fc770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5284, -0.1061],\n",
              "        [-0.5283, -0.1063],\n",
              "        [-0.5283, -0.1063],\n",
              "        [-0.5280, -0.1063],\n",
              "        [-0.5281, -0.1062],\n",
              "        [-0.5280, -0.1063]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Swap the v2's weights to v1\n",
        "\n",
        "# This is to verify the implementaion.\n",
        "\n",
        "sa_v1.wk.data = sa_v2.wk.weight.data.T\n",
        "sa_v1.wq.data = sa_v2.wq.weight.data.T\n",
        "sa_v1.wv.data = sa_v2.wv.weight.data.T\n",
        "\n",
        "assert torch.allclose(sa_v1(inputs), sa_v2(inputs))"
      ],
      "metadata": {
        "id": "FCDMBkZrFs3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Attention\n",
        "\n",
        "Add\n",
        "1. causal mask\n",
        "2. dropout before the softmax (another variant is to apply after the @V is done, but it is less common)"
      ],
      "metadata": {
        "id": "eAPrV472MJl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    \"\"\"Ctor.\n",
        "\n",
        "    Args:\n",
        "      d_in: Input (embedding) dimension.\n",
        "      d_out: Output (hidden) dimension.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.wk = torch.nn.Linear(d_in, d_out, bias=qkv_bias)  # [E, H]\n",
        "    self.wq = torch.nn.Linear(d_in, d_out, bias=qkv_bias)  # [E, H]\n",
        "    self.wv = torch.nn.Linear(d_in, d_out, bias=qkv_bias)  # [E, H]\n",
        "    self.droput = nn.Dropout(p=dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = self.wk(x)   # [N, H]\n",
        "    queries = self.wq(x) # [N, H]\n",
        "    values = self.wv(x)  # [N, H]\n",
        "\n",
        "    attn = keys @ queries.transpose(-1, -2) # [N, N]\n",
        "    # print(f\"initial {attn=}\")\n",
        "\n",
        "    # Make the top triangle -inf so that they will be 0 after softmax\n",
        "    mask = torch.triu(attn, diagonal=1)\n",
        "    attn = attn.masked_fill(self.mask.bool(), -torch.inf)\n",
        "    # print(f\"after causal {attn=}\")\n",
        "\n",
        "    attn /= HIDDEN_DIM ** 0.5\n",
        "    # print(f\"normalized {attn=}\")\n",
        "\n",
        "    attn = nn.functional.softmax(attn, dim=-1)\n",
        "    # print(f\"softmax {attn=}\")\n",
        "\n",
        "    attn = self.droput(attn)\n",
        "    # print(f\"dropout {attn=}\")\n",
        "\n",
        "    res = attn @ values # [N, H]\n",
        "    return res\n",
        "\n",
        "# Test\n",
        "torch.manual_seed(123)\n",
        "sa_v3 = CausalAttention(EMB_DIM, 2, context_length=inputs.shape[-2], dropout=0.5)\n",
        "\n",
        "batch = torch.stack((inputs, inputs))\n",
        "print(f\"{batch.shape=}\")\n",
        "\n",
        "y = sa_v3(batch)\n",
        "print(f\"{y.shape=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4A8NyCYOTAN",
        "outputId": "47473bd5-bbe6-452a-dbfe-c68131b7784f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch.shape=torch.Size([2, 6, 3])\n",
            "y.shape=torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head attention\n",
        "\n",
        "Run multiple attn mechanism multiple times in parallel."
      ],
      "metadata": {
        "id": "L_b_Cx3bZLWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Less efficient implementation\n",
        "\n",
        "Each head matmul is calculated separately"
      ],
      "metadata": {
        "id": "9NqG4fmTmVX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention_v1(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, num_heads, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads!\"\n",
        "    self.heads = nn.ModuleList([CausalAttention(d_in, d_out // num_heads, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], -1)\n",
        "\n",
        "# Test\n",
        "\n",
        "mha = MultiHeadAttention_v1(EMB_DIM, 16, context_length=inputs.shape[-2], num_heads=2, dropout=0.5)\n",
        "y = mha(batch)\n",
        "print(f\"{y.shape=}\")\n",
        "assert y.shape == (batch.shape[0], batch.shape[1], 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61B-7qJjeXz0",
        "outputId": "d1073ddc-f7c4-4646-81ce-e212f55c3de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y.shape=torch.Size([2, 6, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient implementation\n",
        "\n",
        "All heads matmul are combined."
      ],
      "metadata": {
        "id": "qdB7lt3oN6WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, num_heads, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads!\"\n",
        "\n",
        "    self.heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    self.wk = nn.Linear(d_in, d_out, bias=qkv_bias) # [E, H]\n",
        "    self.wq = nn.Linear(d_in, d_out, bias=qkv_bias) # [E, H]\n",
        "    self.wv = nn.Linear(d_in, d_out, bias=qkv_bias) # [E, H]\n",
        "    self.droput = nn.Dropout(p=dropout)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward.\n",
        "\n",
        "    Args:\n",
        "      x: [B, N, E]\n",
        "\n",
        "    Returns:\n",
        "      [B, N, H]\n",
        "    \"\"\"\n",
        "    b, n, d_in = x.shape\n",
        "\n",
        "    k = self.wk(x) # [B, N, H]\n",
        "    q = self.wq(x) # [B, N, H]\n",
        "    v = self.wv(x) # [B, N, H]\n",
        "\n",
        "    k = k.view(b, n, self.heads, self.head_dim).transpose(1, 2) # [B, HEADS, N, HEAD_DIM]\n",
        "    q = q.view(b, n, self.heads, self.head_dim).transpose(1, 2) # [B, HEADS, N, HEAD_DIM]\n",
        "    v = v.view(b, n, self.heads, self.head_dim).transpose(1, 2) # [B, HEADS, N, HEAD_DIM]\n",
        "\n",
        "    attn = q @ k.transpose(-1, -2) # [B, HEADS, N, N]\n",
        "    assert attn.shape == (b, self.heads, n, n)\n",
        "    # print(f\"Before causal: {attn=}\")\n",
        "\n",
        "    # [:n, :n] is to truncate to the length of input tokens.\n",
        "    attn = attn.masked_fill(self.mask.bool()[:n, :n], -torch.inf)\n",
        "    print(f\"After causal: {attn[0][0]=}\")\n",
        "\n",
        "    attn /= self.head_dim ** 0.5\n",
        "    attn = nn.functional.softmax(attn, dim=-1)\n",
        "    print(f\"After softmax: {attn[0][0]=}\")\n",
        "    attn = self.droput(attn)\n",
        "    res = attn @ v # [B, HEADS, N, H]\n",
        "    res = res.transpose(1, 2).contiguous().view(b, n, -1) # [B, N, H]\n",
        "\n",
        "    res = self.out_proj(res)  # [B, N, H]\n",
        "\n",
        "    return res\n",
        "\n",
        "# Test\n",
        "mha = MultiHeadAttention(EMB_DIM, 16, context_length=inputs.shape[-2], num_heads=2, dropout=0.5)\n",
        "y = mha(batch)\n",
        "assert y.shape == (batch.shape[0], batch.shape[1], 16)\n",
        "print(f\"{y.shape=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jIw4sn6OES0",
        "outputId": "c7219c32-7784-45b7-c1dd-42202499658a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After causal: attn[0][0]=tensor([[-0.4106,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.6259, -0.3525,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.6083, -0.3261, -0.3280,    -inf,    -inf,    -inf],\n",
            "        [-0.3780, -0.2722, -0.2715, -0.1127,    -inf,    -inf],\n",
            "        [-0.1209,  0.2420,  0.2340,  0.1942,  0.0236,    -inf],\n",
            "        [-0.5692, -0.5374, -0.5333, -0.2587, -0.3094, -0.3200]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "After softmax: attn[0][0]=tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4759, 0.5241, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3116, 0.3443, 0.3441, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2395, 0.2487, 0.2487, 0.2631, 0.0000, 0.0000],\n",
            "        [0.1838, 0.2090, 0.2084, 0.2055, 0.1934, 0.0000],\n",
            "        [0.1580, 0.1598, 0.1600, 0.1764, 0.1732, 0.1726]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "y.shape=torch.Size([2, 6, 16])\n"
          ]
        }
      ]
    }
  ]
}